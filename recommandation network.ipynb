{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blue      100\n",
       "Purple     33\n",
       "Green      32\n",
       "Red        19\n",
       "Black      17\n",
       "Orange     11\n",
       "Yellow      6\n",
       "Brown       6\n",
       "Grey        4\n",
       "White       4\n",
       "Name: color, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Book2.csv')\n",
    "df = df.drop(columns = ['Location'])\n",
    "df.color.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   gender  color\n",
       "0     26       0   Blue\n",
       "1     22       1    Red\n",
       "2     25       1    Red\n",
       "3     35       0  Black\n",
       "4     27       1   Blue"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'] = pd.factorize(df['gender'])[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  gender\n",
      "0  0.166667     0.0\n",
      "1  0.106061     1.0\n",
      "2  0.151515     1.0\n",
      "3  0.303030     0.0\n",
      "4  0.181818     1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = df.drop(['color'],axis=1)\n",
    "y_data = df['color']\n",
    "MinMaxScaler = preprocessing.MinMaxScaler()\n",
    "X_data_minmax = MinMaxScaler.fit_transform(x_data)\n",
    "data = pd.DataFrame(X_data_minmax,columns=['age','gender'])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y_data,test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Black       0.00      0.00      0.00         3\n",
      "        Blue       0.56      0.69      0.62        26\n",
      "       Green       1.00      0.10      0.18        10\n",
      "      Orange       0.00      0.00      0.00         1\n",
      "      Purple       0.25      0.60      0.35         5\n",
      "         Red       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.47        47\n",
      "   macro avg       0.30      0.23      0.19        47\n",
      "weighted avg       0.55      0.47      0.42        47\n",
      "\n",
      "Accuracy: 0.46808510638297873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "knn_clf=KNeighborsClassifier()\n",
    "knn_clf.fit(X_train,y_train)\n",
    "y_pred=knn_clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "print('Accuracy:',result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Black       0.00      0.00      0.00         3\n",
      "        Blue       0.55      1.00      0.71        26\n",
      "       Green       0.00      0.00      0.00        10\n",
      "      Orange       0.00      0.00      0.00         1\n",
      "      Purple       0.00      0.00      0.00         5\n",
      "         Red       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.55        47\n",
      "   macro avg       0.09      0.17      0.12        47\n",
      "weighted avg       0.31      0.55      0.39        47\n",
      "\n",
      "Accuracy: 0.5531914893617021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC()\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "print('Accuracy:',result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23184355\n",
      "Iteration 2, loss = 2.22304494\n",
      "Iteration 3, loss = 2.21435052\n",
      "Iteration 4, loss = 2.20575782\n",
      "Iteration 5, loss = 2.19725891\n",
      "Iteration 6, loss = 2.18885538\n",
      "Iteration 7, loss = 2.18056273\n",
      "Iteration 8, loss = 2.17236443\n",
      "Iteration 9, loss = 2.16426044\n",
      "Iteration 10, loss = 2.15625285\n",
      "Iteration 11, loss = 2.14834136\n",
      "Iteration 12, loss = 2.14053530\n",
      "Iteration 13, loss = 2.13282815\n",
      "Iteration 14, loss = 2.12520466\n",
      "Iteration 15, loss = 2.11767292\n",
      "Iteration 16, loss = 2.11023310\n",
      "Iteration 17, loss = 2.10288395\n",
      "Iteration 18, loss = 2.09562802\n",
      "Iteration 19, loss = 2.08845647\n",
      "Iteration 20, loss = 2.08136785\n",
      "Iteration 21, loss = 2.07437525\n",
      "Iteration 22, loss = 2.06747870\n",
      "Iteration 23, loss = 2.06068232\n",
      "Iteration 24, loss = 2.05398188\n",
      "Iteration 25, loss = 2.04736937\n",
      "Iteration 26, loss = 2.04085015\n",
      "Iteration 27, loss = 2.03443225\n",
      "Iteration 28, loss = 2.02811648\n",
      "Iteration 29, loss = 2.02189531\n",
      "Iteration 30, loss = 2.01575796\n",
      "Iteration 31, loss = 2.00971708\n",
      "Iteration 32, loss = 2.00377949\n",
      "Iteration 33, loss = 1.99794645\n",
      "Iteration 34, loss = 1.99221470\n",
      "Iteration 35, loss = 1.98657842\n",
      "Iteration 36, loss = 1.98103414\n",
      "Iteration 37, loss = 1.97559664\n",
      "Iteration 38, loss = 1.97026229\n",
      "Iteration 39, loss = 1.96502893\n",
      "Iteration 40, loss = 1.95990235\n",
      "Iteration 41, loss = 1.95489572\n",
      "Iteration 42, loss = 1.94999783\n",
      "Iteration 43, loss = 1.94520438\n",
      "Iteration 44, loss = 1.94051737\n",
      "Iteration 45, loss = 1.93594732\n",
      "Iteration 46, loss = 1.93148652\n",
      "Iteration 47, loss = 1.92713350\n",
      "Iteration 48, loss = 1.92289079\n",
      "Iteration 49, loss = 1.91875831\n",
      "Iteration 50, loss = 1.91473473\n",
      "Iteration 51, loss = 1.91081884\n",
      "Iteration 52, loss = 1.90702098\n",
      "Iteration 53, loss = 1.90333989\n",
      "Iteration 54, loss = 1.89977010\n",
      "Iteration 55, loss = 1.89630028\n",
      "Iteration 56, loss = 1.89293566\n",
      "Iteration 57, loss = 1.88968258\n",
      "Iteration 58, loss = 1.88653036\n",
      "Iteration 59, loss = 1.88347273\n",
      "Iteration 60, loss = 1.88050603\n",
      "Iteration 61, loss = 1.87763550\n",
      "Iteration 62, loss = 1.87485164\n",
      "Iteration 63, loss = 1.87214655\n",
      "Iteration 64, loss = 1.86951836\n",
      "Iteration 65, loss = 1.86696364\n",
      "Iteration 66, loss = 1.86448155\n",
      "Iteration 67, loss = 1.86207058\n",
      "Iteration 68, loss = 1.85972247\n",
      "Iteration 69, loss = 1.85743827\n",
      "Iteration 70, loss = 1.85521387\n",
      "Iteration 71, loss = 1.85304043\n",
      "Iteration 72, loss = 1.85091705\n",
      "Iteration 73, loss = 1.84884342\n",
      "Iteration 74, loss = 1.84681876\n",
      "Iteration 75, loss = 1.84484704\n",
      "Iteration 76, loss = 1.84291693\n",
      "Iteration 77, loss = 1.84102910\n",
      "Iteration 78, loss = 1.83918552\n",
      "Iteration 79, loss = 1.83738155\n",
      "Iteration 80, loss = 1.83561838\n",
      "Iteration 81, loss = 1.83389693\n",
      "Iteration 82, loss = 1.83221155\n",
      "Iteration 83, loss = 1.83056075\n",
      "Iteration 84, loss = 1.82894374\n",
      "Iteration 85, loss = 1.82735670\n",
      "Iteration 86, loss = 1.82580028\n",
      "Iteration 87, loss = 1.82427299\n",
      "Iteration 88, loss = 1.82277453\n",
      "Iteration 89, loss = 1.82130279\n",
      "Iteration 90, loss = 1.81985733\n",
      "Iteration 91, loss = 1.81844074\n",
      "Iteration 92, loss = 1.81705241\n",
      "Iteration 93, loss = 1.81568189\n",
      "Iteration 94, loss = 1.81433151\n",
      "Iteration 95, loss = 1.81300695\n",
      "Iteration 96, loss = 1.81170299\n",
      "Iteration 97, loss = 1.81042084\n",
      "Iteration 98, loss = 1.80916393\n",
      "Iteration 99, loss = 1.80793094\n",
      "Iteration 100, loss = 1.80672046\n",
      "Iteration 101, loss = 1.80553007\n",
      "Iteration 102, loss = 1.80436432\n",
      "Iteration 103, loss = 1.80322306\n",
      "Iteration 104, loss = 1.80210063\n",
      "Iteration 105, loss = 1.80100140\n",
      "Iteration 106, loss = 1.79992236\n",
      "Iteration 107, loss = 1.79886135\n",
      "Iteration 108, loss = 1.79781846\n",
      "Iteration 109, loss = 1.79679264\n",
      "Iteration 110, loss = 1.79578343\n",
      "Iteration 111, loss = 1.79479326\n",
      "Iteration 112, loss = 1.79382027\n",
      "Iteration 113, loss = 1.79286377\n",
      "Iteration 114, loss = 1.79192478\n",
      "Iteration 115, loss = 1.79100276\n",
      "Iteration 116, loss = 1.79009706\n",
      "Iteration 117, loss = 1.78920830\n",
      "Iteration 118, loss = 1.78833413\n",
      "Iteration 119, loss = 1.78747467\n",
      "Iteration 120, loss = 1.78663023\n",
      "Iteration 121, loss = 1.78580113\n",
      "Iteration 122, loss = 1.78498660\n",
      "Iteration 123, loss = 1.78418592\n",
      "Iteration 124, loss = 1.78339888\n",
      "Iteration 125, loss = 1.78262622\n",
      "Iteration 126, loss = 1.78186723\n",
      "Iteration 127, loss = 1.78112271\n",
      "Iteration 128, loss = 1.78039150\n",
      "Iteration 129, loss = 1.77967311\n",
      "Iteration 130, loss = 1.77896664\n",
      "Iteration 131, loss = 1.77827137\n",
      "Iteration 132, loss = 1.77758934\n",
      "Iteration 133, loss = 1.77691854\n",
      "Iteration 134, loss = 1.77625804\n",
      "Iteration 135, loss = 1.77560759\n",
      "Iteration 136, loss = 1.77496711\n",
      "Iteration 137, loss = 1.77433678\n",
      "Iteration 138, loss = 1.77371730\n",
      "Iteration 139, loss = 1.77310840\n",
      "Iteration 140, loss = 1.77250992\n",
      "Iteration 141, loss = 1.77192100\n",
      "Iteration 142, loss = 1.77134207\n",
      "Iteration 143, loss = 1.77077268\n",
      "Iteration 144, loss = 1.77021250\n",
      "Iteration 145, loss = 1.76966103\n",
      "Iteration 146, loss = 1.76911784\n",
      "Iteration 147, loss = 1.76858178\n",
      "Iteration 148, loss = 1.76805344\n",
      "Iteration 149, loss = 1.76753250\n",
      "Iteration 150, loss = 1.76701898\n",
      "Iteration 151, loss = 1.76651248\n",
      "Iteration 152, loss = 1.76601192\n",
      "Iteration 153, loss = 1.76551794\n",
      "Iteration 154, loss = 1.76502974\n",
      "Iteration 155, loss = 1.76454714\n",
      "Iteration 156, loss = 1.76407040\n",
      "Iteration 157, loss = 1.76360034\n",
      "Iteration 158, loss = 1.76313728\n",
      "Iteration 159, loss = 1.76267953\n",
      "Iteration 160, loss = 1.76222532\n",
      "Iteration 161, loss = 1.76177472\n",
      "Iteration 162, loss = 1.76132879\n",
      "Iteration 163, loss = 1.76088777\n",
      "Iteration 164, loss = 1.76045329\n",
      "Iteration 165, loss = 1.76002649\n",
      "Iteration 166, loss = 1.75960430\n",
      "Iteration 167, loss = 1.75918669\n",
      "Iteration 168, loss = 1.75877506\n",
      "Iteration 169, loss = 1.75836471\n",
      "Iteration 170, loss = 1.75795658\n",
      "Iteration 171, loss = 1.75755250\n",
      "Iteration 172, loss = 1.75715395\n",
      "Iteration 173, loss = 1.75676500\n",
      "Iteration 174, loss = 1.75638100\n",
      "Iteration 175, loss = 1.75600047\n",
      "Iteration 176, loss = 1.75562171\n",
      "Iteration 177, loss = 1.75524552\n",
      "Iteration 178, loss = 1.75487255\n",
      "Iteration 179, loss = 1.75450469\n",
      "Iteration 180, loss = 1.75414362\n",
      "Iteration 181, loss = 1.75378646\n",
      "Iteration 182, loss = 1.75343232\n",
      "Iteration 183, loss = 1.75307768\n",
      "Iteration 184, loss = 1.75272447\n",
      "Iteration 185, loss = 1.75237350\n",
      "Iteration 186, loss = 1.75202569\n",
      "Iteration 187, loss = 1.75167872\n",
      "Iteration 188, loss = 1.75133305\n",
      "Iteration 189, loss = 1.75098941\n",
      "Iteration 190, loss = 1.75064928\n",
      "Iteration 191, loss = 1.75031190\n",
      "Iteration 192, loss = 1.74997673\n",
      "Iteration 193, loss = 1.74964096\n",
      "Iteration 194, loss = 1.74930913\n",
      "Iteration 195, loss = 1.74897943\n",
      "Iteration 196, loss = 1.74865199\n",
      "Iteration 197, loss = 1.74832683\n",
      "Iteration 198, loss = 1.74800772\n",
      "Iteration 199, loss = 1.74769100\n",
      "Iteration 200, loss = 1.74737947\n",
      "Iteration 201, loss = 1.74707131\n",
      "Iteration 202, loss = 1.74676554\n",
      "Iteration 203, loss = 1.74645875\n",
      "Iteration 204, loss = 1.74615332\n",
      "Iteration 205, loss = 1.74584950\n",
      "Iteration 206, loss = 1.74554990\n",
      "Iteration 207, loss = 1.74525231\n",
      "Iteration 208, loss = 1.74495561\n",
      "Iteration 209, loss = 1.74466611\n",
      "Iteration 210, loss = 1.74437858\n",
      "Iteration 211, loss = 1.74409116\n",
      "Iteration 212, loss = 1.74380389\n",
      "Iteration 213, loss = 1.74351799\n",
      "Iteration 214, loss = 1.74323415\n",
      "Iteration 215, loss = 1.74294676\n",
      "Iteration 216, loss = 1.74265995\n",
      "Iteration 217, loss = 1.74238058\n",
      "Iteration 218, loss = 1.74210335\n",
      "Iteration 219, loss = 1.74182565\n",
      "Iteration 220, loss = 1.74154941\n",
      "Iteration 221, loss = 1.74127451\n",
      "Iteration 222, loss = 1.74100123\n",
      "Iteration 223, loss = 1.74072908\n",
      "Iteration 224, loss = 1.74045737\n",
      "Iteration 225, loss = 1.74018713\n",
      "Iteration 226, loss = 1.73991845\n",
      "Iteration 227, loss = 1.73965138\n",
      "Iteration 228, loss = 1.73938674\n",
      "Iteration 229, loss = 1.73912309\n",
      "Iteration 230, loss = 1.73885936\n",
      "Iteration 231, loss = 1.73859475\n",
      "Iteration 232, loss = 1.73833032\n",
      "Iteration 233, loss = 1.73806609\n",
      "Iteration 234, loss = 1.73780210\n",
      "Iteration 235, loss = 1.73754129\n",
      "Iteration 236, loss = 1.73728247\n",
      "Iteration 237, loss = 1.73702801\n",
      "Iteration 238, loss = 1.73677478\n",
      "Iteration 239, loss = 1.73652326\n",
      "Iteration 240, loss = 1.73627330\n",
      "Iteration 241, loss = 1.73602402\n",
      "Iteration 242, loss = 1.73577477\n",
      "Iteration 243, loss = 1.73552652\n",
      "Iteration 244, loss = 1.73527784\n",
      "Iteration 245, loss = 1.73502998\n",
      "Iteration 246, loss = 1.73478405\n",
      "Iteration 247, loss = 1.73453962\n",
      "Iteration 248, loss = 1.73429549\n",
      "Iteration 249, loss = 1.73405335\n",
      "Iteration 250, loss = 1.73381187\n",
      "Iteration 251, loss = 1.73357079\n",
      "Iteration 252, loss = 1.73333084\n",
      "Iteration 253, loss = 1.73309258\n",
      "Iteration 254, loss = 1.73285519\n",
      "Iteration 255, loss = 1.73261872\n",
      "Iteration 256, loss = 1.73238290\n",
      "Iteration 257, loss = 1.73214749\n",
      "Iteration 258, loss = 1.73191327\n",
      "Iteration 259, loss = 1.73168050\n",
      "Iteration 260, loss = 1.73144864\n",
      "Iteration 261, loss = 1.73121735\n",
      "Iteration 262, loss = 1.73098616\n",
      "Iteration 263, loss = 1.73075578\n",
      "Iteration 264, loss = 1.73052703\n",
      "Iteration 265, loss = 1.73029926\n",
      "Iteration 266, loss = 1.73007178\n",
      "Iteration 267, loss = 1.72984350\n",
      "Iteration 268, loss = 1.72961654\n",
      "Iteration 269, loss = 1.72939066\n",
      "Iteration 270, loss = 1.72916507\n",
      "Iteration 271, loss = 1.72894056\n",
      "Iteration 272, loss = 1.72871814\n",
      "Iteration 273, loss = 1.72849792\n",
      "Iteration 274, loss = 1.72827993\n",
      "Iteration 275, loss = 1.72806247\n",
      "Iteration 276, loss = 1.72784566\n",
      "Iteration 277, loss = 1.72762993\n",
      "Iteration 278, loss = 1.72741631\n",
      "Iteration 279, loss = 1.72720397\n",
      "Iteration 280, loss = 1.72699216\n",
      "Iteration 281, loss = 1.72678166\n",
      "Iteration 282, loss = 1.72657152\n",
      "Iteration 283, loss = 1.72636231\n",
      "Iteration 284, loss = 1.72615365\n",
      "Iteration 285, loss = 1.72594585\n",
      "Iteration 286, loss = 1.72573855\n",
      "Iteration 287, loss = 1.72553310\n",
      "Iteration 288, loss = 1.72532807\n",
      "Iteration 289, loss = 1.72512325\n",
      "Iteration 290, loss = 1.72492023\n",
      "Iteration 291, loss = 1.72471836\n",
      "Iteration 292, loss = 1.72451691\n",
      "Iteration 293, loss = 1.72431598\n",
      "Iteration 294, loss = 1.72411604\n",
      "Iteration 295, loss = 1.72391592\n",
      "Iteration 296, loss = 1.72371684\n",
      "Iteration 297, loss = 1.72351906\n",
      "Iteration 298, loss = 1.72332156\n",
      "Iteration 299, loss = 1.72312358\n",
      "Iteration 300, loss = 1.72292606\n",
      "Iteration 301, loss = 1.72273010\n",
      "Iteration 302, loss = 1.72253468\n",
      "Iteration 303, loss = 1.72233943\n",
      "Iteration 304, loss = 1.72214553\n",
      "Iteration 305, loss = 1.72195451\n",
      "Iteration 306, loss = 1.72176383\n",
      "Iteration 307, loss = 1.72157442\n",
      "Iteration 308, loss = 1.72138573\n",
      "Iteration 309, loss = 1.72119793\n",
      "Iteration 310, loss = 1.72101105\n",
      "Iteration 311, loss = 1.72082437\n",
      "Iteration 312, loss = 1.72063856\n",
      "Iteration 313, loss = 1.72045324\n",
      "Iteration 314, loss = 1.72026847\n",
      "Iteration 315, loss = 1.72008468\n",
      "Iteration 316, loss = 1.71990132\n",
      "Iteration 317, loss = 1.71971932\n",
      "Iteration 318, loss = 1.71953741\n",
      "Iteration 319, loss = 1.71935648\n",
      "Iteration 320, loss = 1.71917590\n",
      "Iteration 321, loss = 1.71899642\n",
      "Iteration 322, loss = 1.71881699\n",
      "Iteration 323, loss = 1.71863784\n",
      "Iteration 324, loss = 1.71846025\n",
      "Iteration 325, loss = 1.71828351\n",
      "Iteration 326, loss = 1.71810733\n",
      "Iteration 327, loss = 1.71793110\n",
      "Iteration 328, loss = 1.71775564\n",
      "Iteration 329, loss = 1.71758111\n",
      "Iteration 330, loss = 1.71740662\n",
      "Iteration 331, loss = 1.71723364\n",
      "Iteration 332, loss = 1.71706206\n",
      "Iteration 333, loss = 1.71689028\n",
      "Iteration 334, loss = 1.71671785\n",
      "Iteration 335, loss = 1.71654722\n",
      "Iteration 336, loss = 1.71637790\n",
      "Iteration 337, loss = 1.71620890\n",
      "Iteration 338, loss = 1.71603984\n",
      "Iteration 339, loss = 1.71587123\n",
      "Iteration 340, loss = 1.71570336\n",
      "Iteration 341, loss = 1.71553582\n",
      "Iteration 342, loss = 1.71536811\n",
      "Iteration 343, loss = 1.71520162\n",
      "Iteration 344, loss = 1.71503536\n",
      "Iteration 345, loss = 1.71486955\n",
      "Iteration 346, loss = 1.71470612\n",
      "Iteration 347, loss = 1.71454219\n",
      "Iteration 348, loss = 1.71437813\n",
      "Iteration 349, loss = 1.71421527\n",
      "Iteration 350, loss = 1.71405244\n",
      "Iteration 351, loss = 1.71389018\n",
      "Iteration 352, loss = 1.71372915\n",
      "Iteration 353, loss = 1.71356840\n",
      "Iteration 354, loss = 1.71340810\n",
      "Iteration 355, loss = 1.71324763\n",
      "Iteration 356, loss = 1.71309031\n",
      "Iteration 357, loss = 1.71293229\n",
      "Iteration 358, loss = 1.71277354\n",
      "Iteration 359, loss = 1.71261553\n",
      "Iteration 360, loss = 1.71245866\n",
      "Iteration 361, loss = 1.71230304\n",
      "Iteration 362, loss = 1.71214769\n",
      "Iteration 363, loss = 1.71199251\n",
      "Iteration 364, loss = 1.71183761\n",
      "Iteration 365, loss = 1.71168359\n",
      "Iteration 366, loss = 1.71153011\n",
      "Iteration 367, loss = 1.71137662\n",
      "Iteration 368, loss = 1.71122345\n",
      "Iteration 369, loss = 1.71107196\n",
      "Iteration 370, loss = 1.71091962\n",
      "Iteration 371, loss = 1.71076815\n",
      "Iteration 372, loss = 1.71061737\n",
      "Iteration 373, loss = 1.71046732\n",
      "Iteration 374, loss = 1.71031729\n",
      "Iteration 375, loss = 1.71016749\n",
      "Iteration 376, loss = 1.71001841\n",
      "Iteration 377, loss = 1.70987021\n",
      "Iteration 378, loss = 1.70972270\n",
      "Iteration 379, loss = 1.70957452\n",
      "Iteration 380, loss = 1.70942746\n",
      "Iteration 381, loss = 1.70928068\n",
      "Iteration 382, loss = 1.70913561\n",
      "Iteration 383, loss = 1.70899019\n",
      "Iteration 384, loss = 1.70884356\n",
      "Iteration 385, loss = 1.70869864\n",
      "Iteration 386, loss = 1.70855417\n",
      "Iteration 387, loss = 1.70840967\n",
      "Iteration 388, loss = 1.70826555\n",
      "Iteration 389, loss = 1.70812282\n",
      "Iteration 390, loss = 1.70798018\n",
      "Iteration 391, loss = 1.70783746\n",
      "Iteration 392, loss = 1.70769618\n",
      "Iteration 393, loss = 1.70755533\n",
      "Iteration 394, loss = 1.70741376\n",
      "Iteration 395, loss = 1.70727264\n",
      "Iteration 396, loss = 1.70713274\n",
      "Iteration 397, loss = 1.70699379\n",
      "Iteration 398, loss = 1.70685495\n",
      "Iteration 399, loss = 1.70671636\n",
      "Iteration 400, loss = 1.70657792\n",
      "Iteration 401, loss = 1.70644065\n",
      "Iteration 402, loss = 1.70630240\n",
      "Iteration 403, loss = 1.70616519\n",
      "Iteration 404, loss = 1.70602840\n",
      "Iteration 405, loss = 1.70589067\n",
      "Iteration 406, loss = 1.70575341\n",
      "Iteration 407, loss = 1.70561757\n",
      "Iteration 408, loss = 1.70548235\n",
      "Iteration 409, loss = 1.70534676\n",
      "Iteration 410, loss = 1.70521248\n",
      "Iteration 411, loss = 1.70507793\n",
      "Iteration 412, loss = 1.70494402\n",
      "Iteration 413, loss = 1.70481053\n",
      "Iteration 414, loss = 1.70467783\n",
      "Iteration 415, loss = 1.70454667\n",
      "Iteration 416, loss = 1.70441574\n",
      "Iteration 417, loss = 1.70428540\n",
      "Iteration 418, loss = 1.70415539\n",
      "Iteration 419, loss = 1.70402541\n",
      "Iteration 420, loss = 1.70389769\n",
      "Iteration 421, loss = 1.70376968\n",
      "Iteration 422, loss = 1.70364059\n",
      "Iteration 423, loss = 1.70351306\n",
      "Iteration 424, loss = 1.70338692\n",
      "Iteration 425, loss = 1.70326108\n",
      "Iteration 426, loss = 1.70313430\n",
      "Iteration 427, loss = 1.70300853\n",
      "Iteration 428, loss = 1.70288418\n",
      "Iteration 429, loss = 1.70275905\n",
      "Iteration 430, loss = 1.70263464\n",
      "Iteration 431, loss = 1.70251120\n",
      "Iteration 432, loss = 1.70238803\n",
      "Iteration 433, loss = 1.70226487\n",
      "Iteration 434, loss = 1.70214208\n",
      "Iteration 435, loss = 1.70201994\n",
      "Iteration 436, loss = 1.70189761\n",
      "Iteration 437, loss = 1.70177514\n",
      "Iteration 438, loss = 1.70165424\n",
      "Iteration 439, loss = 1.70153380\n",
      "Iteration 440, loss = 1.70141320\n",
      "Iteration 441, loss = 1.70129282\n",
      "Iteration 442, loss = 1.70117299\n",
      "Iteration 443, loss = 1.70105473\n",
      "Iteration 444, loss = 1.70093670\n",
      "Iteration 445, loss = 1.70081808\n",
      "Iteration 446, loss = 1.70069923\n",
      "Iteration 447, loss = 1.70058219\n",
      "Iteration 448, loss = 1.70046456\n",
      "Iteration 449, loss = 1.70034897\n",
      "Iteration 450, loss = 1.70023231\n",
      "Iteration 451, loss = 1.70011583\n",
      "Iteration 452, loss = 1.69999992\n",
      "Iteration 453, loss = 1.69988505\n",
      "Iteration 454, loss = 1.69977034\n",
      "Iteration 455, loss = 1.69965549\n",
      "Iteration 456, loss = 1.69954046\n",
      "Iteration 457, loss = 1.69942672\n",
      "Iteration 458, loss = 1.69931425\n",
      "Iteration 459, loss = 1.69920095\n",
      "Iteration 460, loss = 1.69908790\n",
      "Iteration 461, loss = 1.69897537\n",
      "Iteration 462, loss = 1.69886428\n",
      "Iteration 463, loss = 1.69875303\n",
      "Iteration 464, loss = 1.69864242\n",
      "Iteration 465, loss = 1.69853221\n",
      "Iteration 466, loss = 1.69842129\n",
      "Iteration 467, loss = 1.69831190\n",
      "Iteration 468, loss = 1.69820270\n",
      "Iteration 469, loss = 1.69809233\n",
      "Iteration 470, loss = 1.69798325\n",
      "Iteration 471, loss = 1.69787557\n",
      "Iteration 472, loss = 1.69776760\n",
      "Iteration 473, loss = 1.69765995\n",
      "Iteration 474, loss = 1.69755294\n",
      "Iteration 475, loss = 1.69744582\n",
      "Iteration 476, loss = 1.69733942\n",
      "Iteration 477, loss = 1.69723292\n",
      "Iteration 478, loss = 1.69712655\n",
      "Iteration 479, loss = 1.69702299\n",
      "Iteration 480, loss = 1.69691786\n",
      "Iteration 481, loss = 1.69681224\n",
      "Iteration 482, loss = 1.69670806\n",
      "Iteration 483, loss = 1.69660399\n",
      "Iteration 484, loss = 1.69649964\n",
      "Iteration 485, loss = 1.69639656\n",
      "Iteration 486, loss = 1.69629402\n",
      "Iteration 487, loss = 1.69619138\n",
      "Iteration 488, loss = 1.69608779\n",
      "Iteration 489, loss = 1.69598483\n",
      "Iteration 490, loss = 1.69588393\n",
      "Iteration 491, loss = 1.69578198\n",
      "Iteration 492, loss = 1.69568048\n",
      "Iteration 493, loss = 1.69557891\n",
      "Iteration 494, loss = 1.69547917\n",
      "Iteration 495, loss = 1.69537905\n",
      "Iteration 496, loss = 1.69527895\n",
      "Iteration 497, loss = 1.69517908\n",
      "Iteration 498, loss = 1.69507969\n",
      "Iteration 499, loss = 1.69498011\n",
      "Iteration 500, loss = 1.69488066\n",
      "Iteration 501, loss = 1.69478104\n",
      "Iteration 502, loss = 1.69468236\n",
      "Iteration 503, loss = 1.69458472\n",
      "Iteration 504, loss = 1.69448586\n",
      "Iteration 505, loss = 1.69438801\n",
      "Iteration 506, loss = 1.69429092\n",
      "Iteration 507, loss = 1.69419550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Black       0.00      0.00      0.00         3\n",
      "        Blue       0.57      1.00      0.72        26\n",
      "       Green       0.00      0.00      0.00        10\n",
      "      Orange       0.00      0.00      0.00         1\n",
      "      Purple       0.00      0.00      0.00         5\n",
      "         Red       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.55        47\n",
      "   macro avg       0.09      0.17      0.12        47\n",
      "weighted avg       0.31      0.55      0.40        47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Fady\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_clf = MLPClassifier(max_iter = 1600,activation = 'relu', solver = 'adam', verbose=True)\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "y_pred = mlp_clf.predict(X_test)\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 47 points : 32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f703ccf650467d2787b30a7549937d4a5b4f992dc13ef9b6cc8bda4cf48eca72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
